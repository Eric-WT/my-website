<!DOCTYPE html>
<html>

<head>
    <title>The 1st International Workshop on Multimodal Foundation Models for Spatial Intelligence</title>

    <style>
        body {
            width: 95%;
            margin: 0 auto;
            background-color: #f2f2f2;
            font-size: 18px;
            /* default 16px */
        }

        a:link {
            color: blue;
        }

        a:visited {
            color: blue;
        }

        a:hover {
            color: orange;
        }

        img {
            max-width: 100%;
            max-height: 100%;
        }

        td {
            text-align: left;
            padding-top: 5px;
            padding-bottom: 5px;
            padding-left: 15px;
            padding-right: 0px;
        }

        .row {
            content: "";
            clear: both;
            display: flex;
            flex-direction: row;
            justify-content: center;
            padding-bottom: 5px;
        }

        .column {
            float: left;
            width: 15%;
            padding: 5px;
        }

        .container {
            text-align: center;
            background-color: #ffffff;
            padding: 15px;
            margin: 20px;
        }

        .text-content {
            text-align: left;
        }

        .text-content .text-box {
            text-indent: 2ch;
        }

        .text-content .table-box {
            width: 100%;
            height: 0;
            padding-bottom: 29%;
            background: url(table.jpg) no-repeat;
            background-size: 100% 100%;
        }

        .text-content .text-box .text-title {
            font-weight: bold;
        }

        .text-content .text-box .text-p {
            text-indent: 4ch;
        }

        @media only screen and (max-width: 768px) {
            .row {
                display: flex;
                flex-direction: column;
                justify-content: center;
            }

            .column {
                float: left;
                width: 100%;
                padding: 0px;
                margin-bottom: 10px;
            }
        }
    </style>

    <meta charset="UTF-8">
    <meta name="description" content="ECCV 2024 Tutorial on Emerging Trends in Disentanglement and Compositionality">
    <meta name="keywords" content="ECCV, Disentanglement and Compositionality, Computer Vision">
    <meta name="author" content="Xin Jin et al.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>

    <div class="container">
        <h2>The 1st International Workshop on Multimodal Foundation</h2>
        <h2>Models for Spatial Intelligence</h2>
        <img src="MM25.jpg" width="50%" alt="MM25">
    </div>

    <div class="container">
        <h2>Tutorial lecturers</h2>

        <div class="row">
            <div class="column">
                <a href="https://djiajunustc.github.io/">
                    <img src="djj.jpg" alt="Jiajun Deng" width="220" height="230">
                    <div>Jiajun Deng<br>The University of Adelaide<br>Adelaide, Australia<br>jiajun.deng@adelaide.edu.au
                    </div>
                </a>
            </div>
            <div class="column">
                <a href="https://llijiang.github.io/">
                    <img src="jl.png" alt="Li Jiang" width="220" height="230">
                    <div>Li Jiang<br>CUHK (Shenzhen)<br>Shenzhen, China<br>jiangli@cuhk.edu.cn</div>
                </a>
            </div>
            <div class="column">
                <a href="https://www.eitech.edu.cn/?tid=49&p=teacher&lang=en">
                    <img src="jx.jpg" alt="Xin Jin" width="220" height="230">
                    <div>Xin Jin<br>Eastern Institute of Technology<br>Ningbo, China<br>jinxin@eitech.edu.cn</div>
                </a>
            </div>
            <div class="column">
                <a href="https://www.microsoft.com/en-us/research/people/tianyuhe/">
                    <img src="hty.jpg" alt="Tianyu He" width="220" height="230">
                    <div>Tianyu He<br>Microsoft<br>Beijing, China<br>tianyuhe@microsoft.com</div>
                </a>
            </div>
            <div class="column">
                <a href="https://zhengyuan.info/">
                    <img src="yzy.jpg" alt="Zhengyuan Yang" width="220" height="230">
                    <div>Zhengyuan Yang<br>Microsoft<br>Seattle, USA<br>zhengyuan.yang13@gmail.com</div>
                </a>
            </div>
        </div>

        <div class="row">
            <div class="column">
                <a href="http://staff.ustc.edu.cn/~zhwg/">
                    <img src="zwg.png" alt="Wengang Zhou" width="220" height="230">
                    <div>Wengang Zhou<br>USTC<br>Hefei, China<br>zhwg@ustc.edu.cn</div>
                </a>
            </div>
            <div class="column">
                <a href="https://scholar.google.com.au/citations?user=RMSuNFwAAAAJ&hl=en">
                    <img src="yy.png" alt="Yi Yang" width="220" height="230">
                    <div>Yi Yang<br>Zhejiang University<br>Hangzhou, China<br>yangyics@zju.edu.cn</div>
                </a>
            </div>
            <div class="column">
                <a href="https://disi.unitn.it/~sebe/">
                    <img src="NicuSebe.png" alt="Nicu Sebe" width="220" height="230">
                    <div>Nicu Sebe<br>University of Trento<br>Trento, Italy<br>sebe@disi.unitn.it</div>
                </a>
            </div>
            <div class="column">
                <a href="http://staff.ustc.edu.cn/~lihq/en/">
                    <img src="lhq.jpg" alt="Houqiang Li" width="220" height="230">
                    <div>Houqiang Li<br>USTC<br>Hefei, China<br>lihq@ustc.edu.cn</div>
                </a>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>1 SCOPE AND TOPICS</h2>
        <div class="text-content">
            <div class="text-box">
                Multimodal foundation models, including but not limited to large
                vision-language models and text-to-image/video generation models, have reshaped the field of artificial
                intelligence by learning
                highly scalable and transferable representations from vast amounts
                of data, bringing us to the age of generative AI. These models have
                evolved from static, task-specific architectures to general-purpose
                systems capable of adaptation, knowledge synthesis, multidomain
                understanding, and multimodal content creation. Their ability to
                integrate, process, and produce multimodal data—such as text, images, and audio—enables these models to
                serve as helpful assistants
                in daily work and greatly improve work efficiency. Moreover, the
                rapid development in AI agents based on multimodal foundation
                models suggests that these models are not merely passive tools but
                are increasingly capable of autonomous decision-making, planning,
                and reasoning in interactive environments, which opens the door
                for AI models to participate in the physical world. However, the
                heavy reliance on predominantly linguistic and 2D representations
                limits current multimodal foundation models in physical-world
                applications that require the ability of deep 3D spatial reasoning
                and awareness of physical properties.
            </div>
            <br>

            <div class="text-box">
                It is widely recognized that spatial intelligence plays a crucial
                role in bridging this gap. Defined in cognitive science, spatial intelligence refers to the ability to
                perceive, comprehend, and reason
                about spatial relationships and 3D structures. For our human beings, this capability supports daily
                activities like navigation, driving, and assembling furniture while also being essential for sports,
                STEM fields (architecture, engineering, physics), and creative arts
                (drawing, game design, cinematography). Likewise, by empowering
                to-date AI systems with spatial intelligence, they can gain embodied
                capabilities and effectively interact with the 3D world, which is
                exactly what we expect as the next generation of AI. Specifically,
                achieving spatial intelligence is essential for applications such as
                autonomous vehicles, household robotics, augmented reality, and
                digital twins, where AI models must operate beyond 2D and static data inputs and reason about physical
                constraints, object interactions, and environmental dynamics. Unlike language-based models,
                which rely on discrete tokenized sequences, Spatial Intelligence
                requires understanding continuous, multi-scale representations of
                space and time, probably integrating sensory inputs from sources
                such as depth sensors, videos, and geospatial data.
            </div>
            <br>

            <div class="text-box">
                Despite recent advances, multimodal foundation models still
                face significant challenges in achieving robust spatial intelligence.
                When we talk about this topic, a series of questions come to us:
                <div class="text-p">
                    • How can we extend multimodal foundation models to learn and reason about 3D spaces without
                    extensive labeled data?
                </div>
                <div class="text-p">
                    • How to address the problem of scaling up 3D data? What
                    methods can effectively integrate heterogeneous spatial data
                    sources, such as point clouds, maps, and textual scene descriptions, to enhance robustness?
                </div>
                <div class="text-p">
                    • Are the explicit 3D data or 3D representation really necessary? Can multimodal foundation models
                    also
                    perform
                    geometric reconstruction?
                </div>
                <div class="text-p">
                    • How do we ensure that AI systems operating in physical
                    environments maintain safety, fairness, and explainable?
                </div>
            </div>
            <br>

            <div class="text-box">
                Addressing these challenges is critical for deploying foundation
                models in real-world embodied systems. Within this scope, we believe it’s never excessive to emphasize
                the importance of multimedia
                data and techniques in equipping AI with the ability to process,
                synthesize, and reason about complex spatial environments.
            </div>
            <br>

            <div class="text-box">
                In summary, this workshop seeks to bring together researchers
                and practitioners from multimedia and related communities to
                discuss Multimodal Foundation Models for Spatial Intelligence. There are a lot of open problems to be
                explored, no matter
                the aspects of multimedia data and benchmarks, framework designs, training techniques, or trust-worthy
                algorithms. By uniting
                insights from researchers from various backgrounds, we aim to
                step towards reshaping the future of spatially-aware foundation
                models and paving the way for next-generation AI systems capable
                of perceiving, reasoning, and acting in complex 3D environments.
            </div>
        </div>
    </div>

    <div class="container">
        <h2>2 RATIONALE</h2>
        <div class="text-content">
            <h3>2.1 Broader Impact Statement</h3>
            <div class="text-box">
                At ACM Multimedia 2025, a leading venue for multimedia research,
                this workshop directly aligns with the conference’s focus on multimodal learning, human-centered
                multimedia, spatial computing,
                and cross-modal understanding. Given the increasing role of multimedia data in AI training, adaptation,
                and deployment, our workshop highlights key challenges and opportunities in extending multimodal
                foundation models beyond 2D content to achieve robust
                spatial intelligence.
            </div>
            <br>

            <div class="text-box">
                Our workshop is highly interdisciplinary, bringing together researchers from computer vision, natural
                language processing, robotics, and multimedia research. We aim to bridge the gap between
                foundation model research and real-world spatial AI applications,
                fostering discussions on multimedia data and benchmarks, model
                architectures, learning strategies, and trustworthy algorithms for
                physical environments.
            </div>
            <br>

            <h3>2.2 Target Audience and Workshop Promotion</h3>
            <div class="text-box">
                The potential target audience for this workshop includes the following groups: 1) Researchers &
                Academics – Professors, graduate
                students, and postdocs exploring multimodal foundation models,
                spatial intelligence, and AI for physical environments, seeking insights into recent advancements and
                research challenges. 2) Industry Professionals – AI engineers, data scientists, and robotics experts
                interested in applying multimodal AI to real-world applications like
                autonomous systems and AR/VR. 3) Broader Community – Other
                parties looking to present their work, receive expert feedback, and
                collaborate with peers in the field.
            </div>
            <br>

            <div class="text-box">
                To maximize outreach and engagement of this workshop, we will
                implement the following promotional strategies: 1) Social Media
                Promotion – We will use platforms like Twitter, LinkedIn, and
                Facebook to promote the workshop, share relevant updates, and
                engage with the research community. An event page will be created
                to invite attendees and facilitate discussions. 2) Workshop Website
                – A dedicated website will provide details on the agenda, speakers,
                submission guidelines, and registration. The website link will be
                widely shared on social media and through academic mailing lists.
            </div>
        </div>
    </div>

    <div class="container">
        <h2>3 WORKSHOP DETAILS</h2>
        <div class="text-content">
            <h3>3.1 Paper Submission</h3>
            <div class="text-box">
                <div class="text-title">Paper Track Management.</div>
                <div class="text-box">
                    Our paper track will be managed
                    by four of our organizers, i.e., Dr. Zhengyuan Yang, Prof. Li Jiang,
                    Prof. Wengang Zhou, and Prof. Nicu Sebe. They have successfully
                    served as Area Chairs, Senior Program Chairs, or senior reviewers
                    in top-tier AI and multimedia conferences.
                </div>
            </div>
            <br>

            <div class="text-box">
                <div class="text-title">TOPICS OF INTEREST</div>
                <div class="text-box">
                    We invite submissions of original research contributions related to,
                    but not limited to, the following topics:
                </div>
                <br />
                <div class="text-box">
                    <div class="text-title">1) Multimodal Spatial Understanding</div>

                    <div class="text-p">
                        • Multimodal Large Language Models with Spatial Awareness
                    </div>
                    <div class="text-p">
                        • (3D) Vision-Language-Action Alignment
                    </div>
                    <div class="text-p">
                        • 3D Scene Perception (Detection, Segmentation)
                    </div>
                    <div class="text-p">
                        • 3D Semantic Occupancy Prediction
                    </div>
                    <div class="text-p">
                        • Multimodal Spatial Reasoning (Images, Videos, Point Clouds,
                        Text, Audio, etc.)
                    </div>
                    <div class="text-p">
                        • 3D Spatial Grounding
                    </div>
                    <div class="text-p">
                        • Multimodal Affordance Learning
                    </div>
                </div>
                <div class="text-box">
                    <div class="text-title">2) 3D/3D-Aware Generative Models and World Models</div>
                    <div class="text-p">
                        • 3D-Aware Diffusion Models and Variational Autoencoders
                        (VAEs)
                    </div>
                    <div class="text-p">
                        • World Models and Its Application in Embodied AI and Autonomous Vehicles
                    </div>
                    <div class="text-p">
                        • 3D Generative Adversarial Networks (3D GANs)
                    </div>
                    <div class="text-p">
                        • Multi-view Consistent 3D Generation
                    </div>
                    <div class="text-p">
                        • Camera View and Motion Controllability in Generation
                    </div>
                </div>
                <div class="text-box">
                    <div class="text-title">3) 3D Geometric Reconstruction</div>
                    <div class="text-p">
                        • 3D Reconstruction from Multimodal Inputs
                    </div>
                    <div class="text-p">
                        • Neural Implicit Representations for 3D Reconstruction
                    </div>
                    <div class="text-p">
                        • 3D Gaussian Splatting for High-fidelity Scene Reconstruction
                    </div>
                    <div class="text-p">
                        • Integration of Geometric Priors in Deep Learning Models
                    </div>
                    <div class="text-p">
                        • SLAM and Semantic SLAM
                    </div>
                    <div class="text-p">
                        • Scalable 3D Reconstruction for Large-scale Datasets
                    </div>
                </div>
                <div class="text-box">
                    <div class="text-title">4) Data and Benchmarks for Deep Spatial
                        Analysis</div>
                    <div class="text-p">
                        • Benchmarks for Spatial Intelligence
                    </div>
                    <div class="text-p">
                        • Large-scale Datasets for Multimodal Spatial Reasoning
                    </div>
                    <div class="text-p">
                        • Standardized Evaluation Protocols for 3D Generative Models
                    </div>
                    <div class="text-p">
                        • Novel Multimodal Annotation Techniques and Crowdsourced
                        Datasets
                    </div>
                    <div class="text-p">
                        • Multimodal Learning for World Simulation
                    </div>
                </div>
                <div class="text-box">
                    <div class="text-title">5) Trustworthy Spatial Intelligence</div>
                    <div class="text-p">
                        • Ethical Considerations in 3D Generative Models
                    </div>
                    <div class="text-p">
                        • Trustworthy and Robust 3D Foundation Models
                    </div>
                    <div class="text-p">
                        • Fairness and Bias in 3D and Multimodal Datasets and Foundation Models
                    </div>

                    <div class="text-box">
                        <div class="text-box"><span class="text-title">Submission Format. </span>We welcome three types
                            of submissions, all of which should align with the topics of interest above:
                        </div>
                        <div class="text-p">
                            • Position or Perspective Papers (up to 4 pages, excluding references): Original ideas,
                            perspectives, research visions, and
                            open challenges related to evaluation approaches for explainable recommender systems.
                        </div>
                        <div class="text-p">
                            • Featured Papers(title, abstract, and the original paper): Previously published papers or
                            summaries of existing publications
                            from leading conferences and high-impact journals that are
                            relevant to the workshop theme.
                        </div>
                        <div class="text-p">
                            • Demonstration Papers (up to 2 pages, excluding references):
                            Original or previously published prototypes and operational
                            evaluation approaches in explainable recommender systems.
                        </div>
                    </div>

                </div>
                <br>

                <div class="text-box">
                    Page limits include diagrams and appendices. Submissions should
                    be double-blind, written in English, and formatted according to the current ACM two-column
                    conference format. Suitable LaTeX and
                    Word templates are available from the ACM Website, specifically,
                    the authors are supposed to use “sigconf” proceedings template for
                    LaTeX and the Interim Template for Word.
                </div>

            </div>
            <br>
            <div class="table-box"></div>
            <br>

            <h3>3.2 Potential Keynote Speakers</h3>
            <div class="text-box">
                <span class="text-title">Tao Mei</span> is the Founder and CEO of HiDream.ai, specializing in
                multimedia analysis and generative intelligence for creativity. A
                globally recognized expert in AI, computer vision, and multimedia,
                he is an International Fellow of the Canadian Academy of Engineering, a Fellow of IEEE, IAPR, and CAAI,
                and a Distinguished
                Scientist of ACM. With over 40,000 citations (h-index 100), 15 paper
                awards, and 70+ granted patents, his research has driven the development of widely used commercial
                products. Previously, Dr. Mei
                served as Vice President of JD.COM and Senior Research Manager
                at Microsoft Research. He has held key leadership roles, including
                Vice Chair of the IEEE SPS IVMSP Technical Committee, member
                of the IEEE Fellow Committee, Executive Committee Member of
                ACM SIGMM, and Founding Co-chair of ACM Multimedia Asia.
                Additionally, he has contributed as a Senior or Associate Editor
                for multiple IEEE/ACM Transactions and as a General/Program
                Co-chair for ACM Multimedia and IEEE ICME.
            </div>

            <div class="text-box">
                <span class="text-title">Mingming Cheng</span> he is a full professor at Nankai University since
                2016, leading the Media Computing Lab. He received his PhD degree from Tsinghua University in 2012, and
                then worked with Prof.
                Philip Torr in Oxford at Oxford University for 2 years. His research
                interests include computer vision and computer graphics. He received several awards, including ACM
                China Rising Star Award,
                IBM Global SUR Award, etc. He is a senior member of the IEEE and
                on the editorial boards of IEEE TPAMI and IEEE TIP.
            </div>

            <div class="text-box">
                <span class="text-title">Saining Xie</span> is an assistant professor at New York University.
                Previously, he was a research scientist at Facebook AI Research (FAIR).
                He received his Ph.D. and M.S. degrees in computer science from
                the University of California San Diego, advised by Zhuowen Tu.
                Prior to that, he received his Bachelor’s degree from Shanghai Jiao
                Tong University. He has broad research interests in deep learning and computer vision, with a focus on
                developing deep representation learning techniques to push the boundaries of core visual
                recognition. He is a recipient of the Marr Prize Honorable Mention
                at ICCV 2015.
            </div>

            <div class="text-box">
                <span class="text-title">Vladislav Golyanik</span> is a research group leader and principal
                investigator at Max Planck Institute for Informatics. He received his Ph.D.
                degree from the University of Kaiserslautern. His research focus
                lies on 3D reconstruction and analysis of general deformable scenes,
                3D reconstruction of the human body and matching problems on
                point sets and graphs. He is interested in neural approaches (both
                supervised and unsupervised), physics-based methods as well as
                new hardware and sensors (, quantum computers and event cameras). His work received the Best Student
                Paper Award at 3DV 2022
                and the Best Paper Award at WACV 2016.
            </div>

            <h3>3.3 Workshop Schedule</h3>
            <div class="text-box">
                We plan to organize the workshop in a hybrid format, accommodating both on-site and online
                participation. For the on-site component,
                at least two organizers will be present to host the event in person.
                The workshop will feature two primary activities: invited keynote
                speeches and paper presentations.
            </div>

            <div class="text-box">
                The workshop will begin with a half-day of keynote presentations delivered by experts from various
                global organizations who
                are leaders in the relevant field. This will be followed by sessions
                dedicated to the presentation of accepted papers. These sessions
                aim to foster an environment for sharing cutting-edge research
                and facilitating meaningful discussions among participants. The
                detailed schedule of the workshop activities is outlined in Table 1.
            </div>

        </div>
    </div>

    <div class="container">
        <h2>4 ORGANIZERS</h2>
        <div class="text-content">
            <div class="text-box">
                Our organizing team has extensive experience organizing special
                issues, workshops, and tutorials at top-tier journals and conferences, as well as a successful and
                relevant paper track. Our team is
                comprised of diverse individuals from academia and industry, at
                varying career stages and from different countries. The organizers
                and their biographies are listed below.
            </div>
        </div>
        <br>
    </div>
    <br>

</body>

</html>